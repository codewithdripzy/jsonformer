{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codewithdripzy/jsonformer/blob/main/Jsonformer_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-kogdhHO3PC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate jsonformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y0Jjj0sOtor",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "model_name = \"databricks/dolly-v2-3b\"\n",
        "\n",
        "offload_folder = \"offload\";\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_cache=True, device_map=\"auto\", offload_folder=offload_folder)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_cache=True)\n",
        "print(\"Loaded model and tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache();"
      ],
      "metadata": {
        "id": "jIbFxsUOZ-oS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E7zFQ3NO0Yd",
        "outputId": "9029d50e-1323-40be-9645-bd480b50684b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating...\n"
          ]
        }
      ],
      "source": [
        "from jsonformer.format import highlight_values\n",
        "from jsonformer.main import Jsonformer\n",
        "import torch # import torch for memory management\n",
        "\n",
        "intents = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"prompt\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"possible_intents\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"string\"\n",
        "            }\n",
        "        },\n",
        "        \"predicted_intent\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"correct_intent\": {\n",
        "            \"type\": \"string\"\n",
        "        },\n",
        "        \"is_correct\": {\n",
        "            \"type\": \"boolean\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "Generate high-quality data for training a natural language understanding (NLU) model.\n",
        "Each generated sample should contain the following information:\n",
        "- A natural language prompt or question from a user.\n",
        "- A list of possible intents the NLU model might classify, written as verbs or action phrases in verb form,\n",
        "  either a single word (e.g., 'add', 'delete') or two words connected by an underscore (e.g., 'add_item', 'get_info').\n",
        "- The predicted intent should also follow the same verb-based structure.\n",
        "- The correct intent, which is the true action or intent the user expects.\n",
        "- A boolean value that indicates whether the predicted intent matches the correct intent (true or false).\n",
        "\n",
        "Rules:\n",
        "- The intents should always be verbs (action-based), following the structure of either single words or compound words with underscores.\n",
        "- Prompts should be diverse and relevant to common actions or tasks.\n",
        "- Ensure that there is a mix of correct and incorrect predictions to simulate real-world model performance.\n",
        "\n",
        "Here is the schema:\n",
        "\"\"\"\n",
        "\n",
        "builder = Jsonformer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    json_schema=intents,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "print(\"Generating...\")\n",
        "# Try offloading the model to the CPU\n",
        "model.to('cpu')\n",
        "# Or be more specific for the device map\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, use_cache=True, device_map={'':0}, offload_folder=offload_folder)\n",
        "output = builder()\n",
        "# Empty cache after generation\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "highlight_values(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store the generated data\n",
        "\n",
        "import json\n",
        "\n",
        "# Save generated data to a JSON file\n",
        "with open(\"generated_nlu_data_verb_intents.json\", \"w\") as file:\n",
        "    json.dump(output, file, indent=4)\n",
        "\n",
        "print(\"Data saved successfully!\");"
      ],
      "metadata": {
        "id": "0eA8hzFHA1jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}